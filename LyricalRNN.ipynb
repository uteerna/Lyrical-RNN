{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrical RNN\n",
    "### JT Wolohan, Uteerna Koul and Paritosh Prakash\n",
    "\n",
    "**{jwolohan, ukoul, pmorpari}@indiana.edu**\n",
    "\n",
    "*[Copyright (c) 2018 - Mozilla Public License v. 2.0](https://www.mozilla.org/en-US/MPL/2.0/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Imports and data loading\n",
    "We're just bringing in the data we're going to need later and some modules for our data loading steps.\n",
    "The actual modeling is **way** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv, random, pickle\n",
    "import spacy\n",
    "from gensim.models.fasttext import FastText\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import nltk\n",
    "from scipy.spatial.distance import cosine as cossim\n",
    "from math import log10\n",
    "import Lyrics2Vectors as l2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyricVectors = FastText.load(\"LyricVectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titlesAndLyrics = l2v.loadTitlesLyrics()\n",
    "#lyr_idfs = pickle.load(open(\"LyricTokenIDFs.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lyr2Mat Class\n",
    "This class is the workhorse of our preprocessing efforts. Feed this class with a spacy NLP model, a word vector model, and a dict of IDF scores from the training data and it'll be able to produce input matricies for use in the sequence to sequence RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Lyr2Mat:\n",
    "  def __init__(self,nlp,vecs,idfs):\n",
    "    self._nlp = nlp\n",
    "    self._wv = vecs\n",
    "    self._idfs = idfs\n",
    "    self._lyrics_tf = {}\n",
    "    self.x_pad = 300\n",
    "    self.y_pad = 10\n",
    "  def _cleanLookup(self,tkn):\n",
    "    if tkn in self._wv.vocab.keys():\n",
    "      v = self._wv[tkn]\n",
    "    else:\n",
    "      v = np.zeros(100)\n",
    "    return v\n",
    "  def vectorize(self,token):\n",
    "    \"\"\"Turns token into wordspace, pos, tf, idf vector\"\"\"\n",
    "    tkn = token\n",
    "    idf = np.array(self._idfs.get(tkn,0))\n",
    "    tf = np.array(log10(1+self._lyrics_tf.get(tkn,0)))\n",
    "    v = self._cleanLookup(tkn)\n",
    "    return np.hstack((v,tf,idf))\n",
    "  def title2Seq(self,title):\n",
    "    \"\"\"Converts a title to a title matrix\"\"\"\n",
    "    M = np.full((self.y_pad,100),0.0)\n",
    "    for i,tkn in enumerate(nltk.tokenize.word_tokenize(title)):\n",
    "      if i < self.y_pad:\n",
    "        M[i] = self._cleanLookup(tkn)\n",
    "      else:\n",
    "        return M\n",
    "    return M\n",
    "  def decodeMatrix(self,M):\n",
    "    output = []\n",
    "    for row in M:\n",
    "      new,dist = self._wv.similar_by_vector(row,topn=1)[0]\n",
    "      if dist >= .39:\n",
    "        output.append(new)\n",
    "      else:\n",
    "        return \" \".join(output)\n",
    "  def creatify(self,words,creative=\"happy\"):\n",
    "    return \" \".join([lyricVectors.wv.most_similar(positive=[word,creative])[0][0] for word in words.split()])\n",
    "  def createInputMatrix(self,lyrics):\n",
    "    \"\"\"Converts lyrics into input matrix\"\"\"\n",
    "    M = np.full((self.x_pad,102),0.0)\n",
    "    tokens = nltk.tokenize.word_tokenize(lyrics)\n",
    "    self._lyrics_tf = {x:tokens.count(x) for x in set(tokens)}\n",
    "    for i,token in enumerate(tokens):\n",
    "      if i < self.x_pad:\n",
    "        M[i] = self.vectorize(token)\n",
    "      else:\n",
    "        return M\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lyr2Mat example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "N = 100\n",
    "nums = random.sample(range(len(titlesAndLyrics['Lyrics'])),N)\n",
    "#L2M = Lyr2Mat(_nlp,lyricVectors.wv,lyr_idfs)\n",
    "lyrics = [titlesAndLyrics['Lyrics'][i][:100] for i in nums]\n",
    "titles = [\"strt \"+ titlesAndLyrics['Titles'][i] + \" stp\" for i in nums]\n",
    "#inputs = np.array([L2M.createInputMatrix(lyrics[i]) for i in range(N)])\n",
    "#targets = np.array([L2M.title2Seq(titles[i]) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Seq2Seq RNN begins here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100\n",
      "Number of unique input tokens: 826\n",
      "Number of unique output tokens: 223\n",
      "Max sequence length for inputs: 100\n",
      "Max sequence length for outputs: 46\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/4\n",
      "80/80 [==============================] - 5s 63ms/step - loss: 0.5003 - val_loss: 0.5951\n",
      "Epoch 2/4\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4695 - val_loss: 0.5368\n",
      "Epoch 3/4\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4195 - val_loss: 0.5345\n",
      "Epoch 4/4\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.4097 - val_loss: 0.5356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe02bf82ba8>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 25  # Batch size for training.\n",
    "epochs = 4  # Number of epochs to train for.\n",
    "latent_dim = 100  # Latent dimensionality of the encoding space.\n",
    "num_samples = N # Number of samples to train on.\n",
    "\n",
    "def flattenWords(left,right):\n",
    "  return left + nltk.tokenize.word_tokenize(right)\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = lyrics[:num_samples]\n",
    "target_texts = titles[:num_samples]\n",
    "input_characters = set(functools.reduce(flattenWords,input_texts,[]))\n",
    "target_characters = set(functools.reduce(flattenWords,target_texts,[]))\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(nltk.tokenize.word_tokenize(input_text)):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(nltk.tokenize.word_tokenize(target_text)):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "#model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strt Kaddish stp                                  Feels ? ' Help stp \n",
      "strt Ain't That So stp                            Goodbye Call Never stp \n",
      "strt Barstool And Dreamers stp                    Strictly But Can stp \n",
      "strt All Tore Up stp                              Celebrate Trouble stp \n",
      "strt California stp                               Call Shape Sparks stp \n",
      "strt Any Other Day stp                            Downtown Save Gone stp \n",
      "strt I Hear Music stp                             've Matter Searching stp \n",
      "strt Give It Up stp                               Soul In About Diggin stp \n",
      "strt In the Morning stp                           Doubt Sera Feeling stp \n",
      "strt Bitches stp                                  Gone Catch Dog stp \n",
      "strt Do What You Like stp                         Has Diggin Can stp \n",
      "strt Searching stp                                Keeps Loving Heart stp \n",
      "strt Dirty Dancer stp                             Nobody Broken Hymn stp \n",
      "strt Master Of Sparks stp                         Nobody Tore 'm stp \n",
      "strt You're Just A Country Boy stp                Can Or 'd Still stp \n",
      "strt It's Never Too Late stp                      I Talking Wan Celebrate stp \n",
      "strt Unforgettable stp                            Ca 're 've When stp \n",
      "strt Mister Superstar stp                         Giving Moon ' ? stp \n",
      "strt Tennessee Waltz stp                          Again Town Bitches stp \n",
      "strt Modern World stp                             Now Ring But Trouble stp \n",
      "strt Charmed Kozaks Life stp                      California Songbird stp \n",
      "strt The Love You Save stp                        Unforgettable Hey stp \n",
      "strt Heart Of Hearts stp                          Unforgettable 've stp \n",
      "strt I Can Hear Your Heartbeat stp                Keeps Late None stp \n",
      "strt He Ain't Give You None stp                   Still Good Swim stp \n",
      "strt If I Can Make Mississippi stp                Dream Broken Heart stp \n",
      "strt I'd Like To Help You Out stp                 Moon Days My Is stp \n",
      "strt I'm A Weight Watcher stp                     And Borrowed Jupiter stp \n",
      "strt I Forgot That Love Existed stp               Nobody Sister Guilty stp \n",
      "strt Sweet Song stp                               Barstool Last Late stp \n",
      "strt I Wanna Be Your Dog stp                      Downtown Marker stp \n",
      "strt It's Just A Matter Of Time stp               Hey Diggin In Doubt stp \n",
      "strt Gossip stp                                   Show Matter In stp \n",
      "strt Heart Of Stone stp                           One Amazing Word stp \n",
      "strt Before It Explodes stp                       Dreamers Face Mummy stp \n",
      "strt What Child Is This? stp                      Hearts Nectar 's stp \n",
      "strt Everybody's Got A Cousin In Miami stp        Rage Ring Trouble stp \n",
      "strt Battle Hymn Of The Republic stp              Massacre Dog Face stp \n",
      "strt A Boy Named Sue stp                          A Ring Giving the stp \n",
      "strt Mood Ring stp                                Waltz If Searching stp \n",
      "strt Celebrate stp                                Before My n't Again stp \n",
      "strt Last Night Again stp                         Business Swim If stp \n",
      "strt Talking To The Moon About You stp            've Too Keeps These stp \n",
      "strt I Know A Dream When I See One stp            the If Last Be stp \n",
      "strt Funny Face stp                               Swim Lover Rage stp \n",
      "strt Neighborhood Threat stp                      Loving Can Catch stp \n",
      "strt Choosey Lover stp                            Gossip Massacre stp \n",
      "strt Jupiter Rising stp                           Just Call Out strt stp \n",
      "strt Pods And Gods stp                            Feeling Told Modern stp \n",
      "strt Guilty stp                                   Bitches Heartbeat stp \n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['strt']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        if sampled_char==\"stp\" and len(decoded_sentence)<15:\n",
    "            sampled_char=random.choice(list(target_token_index.keys()))\n",
    "        decoded_sentence += sampled_char+\" \"\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == 'stp' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(50):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index+2]\n",
    "    decoded_sentence = decode_sequence(input_seq )\n",
    "    #print('Input sentence:', input_texts[seq_index])\n",
    "    print(f\"{titles[seq_index]:<50}{decoded_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
